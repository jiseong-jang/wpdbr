{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4a5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf3749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!d\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dba092",
   "metadata": {},
   "source": [
    "\n",
    "* 우리는 Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes 등을 지원합니다.\n",
    "* 우리는 16비트 LoRA 또는 4비트 QLoRA를 지원합니다. 둘 다 2배 빠릅니다.\n",
    "* `max_seq_length`는 무엇이든 설정할 수 있으며, [kaiokendev의](https://kaiokendev.github.io/til) 방법을 통해 자동 RoPE 스케일링을 수행합니다.\n",
    "* [**NEW**] 우리는 Gemma-2 9b / 27b를 **2배 빠르게** 만듭니다! 우리의 [Gemma-2 9b 노트북](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)을 참조하세요.\n",
    "* [**NEW**] Ollama로 파인튜닝하고 자동으로 내보내려면 우리의 [Ollama 노트북](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)을 사용해보세요.\n",
    "* [**NEW**] 우리는 Mistral NeMo 12B를 2배 빠르게 만들고 12GB 이하의 VRAM에 맞춥니다! [Mistral NeMo 노트북](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)을 참조하세요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f393080",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!d\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a498946",
   "metadata": {},
   "source": [
    "\n",
    "이제 LoRA 어댑터를 추가하여 전체 매개변수의 1~10%만 업데이트하면 됩니다!\n",
    "## LoRA(Low-Rank Adaptation)\n",
    "<img src=\"https://blog.kakaocdn.net/dn/5hM9Y/btsEf71AKDs/PHhKnMg6mQFSCE1BsnSat1/img.png\">\n",
    "\n",
    "\n",
    "- [Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
    "\n",
    "## 주요 개념 설명:\n",
    "\n",
    "1. PEFT (Parameter-Efficient Fine-Tuning):\n",
    "   - 대규모 언어 모델의 효율적인 미세 조정 기법\n",
    "   - 전체 모델 대신 일부 파라미터만 조정하여 학습 효율성 향상\n",
    "\n",
    "2. LoRA (Low-Rank Adaptation):\n",
    "   - PEFT의 한 방법으로, 적은 수의 훈련 가능한 파라미터를 추가하여 모델 미세 조정\n",
    "   - 'r' 파라미터로 LoRA의 랭크 지정\n",
    "\n",
    "3. Gradient Checkpointing:\n",
    "   - 긴 시퀀스 처리 시 메모리 사용을 최적화하는 기법\n",
    "   - \"unsloth\" 옵션으로 더 나은 메모리 효율성 제공\n",
    "\n",
    "4. RS-LoRA (Rank-Stabilized LoRA):\n",
    "   - LoRA의 변형으로, 학습 안정성 개선\n",
    "\n",
    "5. LoftQ:\n",
    "   - 모델 압축을 위한 양자화 기법\n",
    "\n",
    "6. FastLanguageModel:\n",
    "   - 최적화된 언어 모델 처리를 위한 사용자 정의 클래스\n",
    "   - get_peft_model 메소드로 PEFT 설정 적용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47cc41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # 0보다 큰 숫자를 선택하세요! 권장값: 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # 어떤 값이든 지원하지만, 0이 최적화되어 있습니다\n",
    "    bias = \"none\",    # 어떤 값이든 지원하지만, \"none\"이 최적화되어 있습니다\n",
    "    # [새로운 기능] \"unsloth\"는 30% 더 적은 VRAM을 사용하며, 2배 더 큰 배치 크기를 지원합니다!\n",
    "    use_gradient_checkpointing = \"unsloth\", # 매우 긴 컨텍스트의 경우 True 또는 \"unsloth\" 사용\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # 순위 안정화 LoRA를 지원합니다\n",
    "    loftq_config = None, # 그리고 LoftQ도 지원합니다\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1018b32",
   "metadata": {},
   "source": [
    "\n",
    "### 데이터 준비\n",
    "이제 한국어 데이터셋으로 [korean_safe_conversation](https://huggingface.co/datasets/jojo0217/korean_safe_conversation)을 사용합니다.\n",
    "\n",
    "**[참고]** 사용자의 입력을 무시하고 완료된 부분만 학습하려면 TRL의 문서를 [여기](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only)에서 읽어보세요.\n",
    "\n",
    "**[참고]** 토큰화된 출력에 **EOS_TOKEN**을 추가하는 것을 잊지 마세요!! 그렇지 않으면 무한 생성이 발생할 수 있습니다!\n",
    "\n",
    "ShareGPT 데이터셋에 `llama-3` 템플릿을 사용하려면 우리의 대화형 [노트북](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing)을 참조하세요.\n",
    "\n",
    "소설 작성과 같은 텍스트 완성을 위해서는 이 [노트북](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)을 시도해보세요.\n",
    "\n",
    "\n",
    "## 주요 개념 설명:\n",
    "\n",
    "1. 프롬프트 엔지니어링:\n",
    "   - 언어 모델에 특정 형식의 입력을 제공하여 원하는 출력을 유도하는 기법\n",
    "   - 여기서는 Alpaca 형식의 프롬프트를 사용하여 지시사항, 입력, 출력을 구조화\n",
    "\n",
    "2. 데이터셋 전처리:\n",
    "   - 원시 데이터를 모델 학습에 적합한 형태로 변환하는 과정\n",
    "   - formatting_prompts_func 함수를 사용하여 데이터셋의 각 예제를 프롬프트 형식으로 변환\n",
    "\n",
    "3. Hugging Face Datasets:\n",
    "   - 다양한 NLP 데이터셋을 쉽게 로드하고 처리할 수 있게 해주는 라이브러리\n",
    "   - load_dataset 함수로 데이터셋을 로드하고, map 메소드로 전처리 함수를 적용\n",
    "\n",
    "4. EOS 토큰:\n",
    "   - End of Sequence 토큰으로, 텍스트 생성의 종료 시점을 모델에게 알려주는 역할\n",
    "   - 무한 생성을 방지하기 위해 각 예제의 끝에 추가\n",
    "\n",
    "5. 배치 처리:\n",
    "   - map 함수의 batched=True 옵션을 사용하여 데이터를 배치 단위로 처리\n",
    "   - 대량의 데이터를 효율적으로 처리할 수 있게 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpaca_prompt = \"\"\"아래에는 작업을 설명하는 지시 사항과 추가적인 컨텍스트를 제공하는 입력이 있습니다. 요청을 적절하게 완료하는 응답을 작성하세요.\n",
    "\n",
    "### instruction:\n",
    "{}\n",
    "\n",
    "### input:\n",
    "{}\n",
    "\n",
    "### output:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "# 한국어 데이터셋 로드\n",
    "dataset = load_dataset(\"jojo0217/korean_safe_conversation\", split = \"train\")\n",
    "# 데이터셋에 프롬프트 포맷팅 함수 적용\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7cfaf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4024956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "# inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def chat(instruction, input_text=\"\"):\n",
    "    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{0}\n",
    "\n",
    "### Input:\n",
    "{1}\n",
    "\n",
    "### Response:\n",
    "{2}\"\"\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            alpaca_prompt.format(\n",
    "                instruction,  # instruction\n",
    "                input_text,   # input\n",
    "                \"\",           # output - leave this blank for generation!\n",
    "            )\n",
    "        ],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=1024)\n",
    "\n",
    "print(\"채팅을 시작합니다. 종료하려면 'quit'를 입력하세요.\")\n",
    "while True:\n",
    "    instruction = input(\"지시사항을 입력하세요: \")\n",
    "    if instruction.lower() == 'quit':\n",
    "        break\n",
    "    input_text = input(\"추가 입력(없으면 Enter): \")\n",
    "    print(\"\\nAI 응답:\")\n",
    "    chat(instruction, input_text)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"채팅을 종료합니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 모델 학습\n",
    "이제 Huggingface TRL의 `SFTTrainer`를 사용해 봅시다! 자세한 문서는 여기에서 확인하세요: [TRL SFT 문서](https://huggingface.co/docs/trl/sft_trainer).\n",
    "\n",
    "속도를 높이기 위해 60단계로 설정했지만, 전체 실행을 위해 `num_train_epochs=1`로 설정하고 `max_steps=None`을 끌 수 있습니다.\n",
    "\n",
    "우리는 또한 TRL의 `DPOTrainer`를 지원합니다!\n",
    "\n",
    "\n",
    "## 코드에 사용된 개념 설명\n",
    "1. SFT Trainer (Supervised Fine-Tuning Trainer):\n",
    "   - 지도 학습 기반의 미세 조정을 위한 훈련기\n",
    "   - 사전 훈련된 모델을 특정 태스크에 맞게 조정하는 데 사용\n",
    "   - 허깅페이스에서 제공하는 TRL 라이브러리 사용\n",
    "\n",
    "2. TrainingArguments:\n",
    "   - 훈련 과정을 세밀하게 제어하기 위한 다양한 하이퍼파라미터 설정\n",
    "   - 배치 크기, 학습률, 최적화 방법 등을 지정\n",
    "\n",
    "3. 그래디언트 누적 (Gradient Accumulation):\n",
    "   - 여러 배치의 그래디언트를 누적한 후 한 번에 가중치 업데이트\n",
    "   - 메모리 효율성을 높이고 더 큰 배치 효과를 얻을 수 있음\n",
    "\n",
    "4. 학습률 스케줄링 (Learning Rate Scheduling):\n",
    "   - 학습 과정에서 학습률을 동적으로 조절하는 기법\n",
    "   - 여기서는 선형 스케줄링을 사용\n",
    "\n",
    "5. 혼합 정밀도 훈련 (Mixed Precision Training):\n",
    "   - fp16 또는 bfloat16을 사용하여 메모리 사용량 감소 및 연산 속도 향상\n",
    "   - 하드웨어 지원 여부에 따라 자동으로 선택\n",
    "\n",
    "6. 옵티마이저 (Optimizer):\n",
    "   - adamw_8bit: 메모리 효율적인 Adam의 변형\n",
    "   - 가중치 감소(weight decay)를 통한 정규화 적용\n",
    "\n",
    "7. 시드 설정 (Seed):\n",
    "   - 실험의 재현성을 보장하기 위해 난수 생성 시드 고정\n",
    "\n",
    "8. 로깅 (Logging):\n",
    "   - 훈련 과정을 모니터링하기 위한 로그 기록 주기 설정\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # 짧은 시퀀스의 경우 학습 속도를 5배 빠르게 할 수 있습니다.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2, # 각 장치별 배치 크기를 2로 설정합니다.\n",
    "        gradient_accumulation_steps = 4, # 그래디언트 누적 단계를 4로 설정합니다.\n",
    "        warmup_steps = 5, # 워밍업 단계를 5로 설정합니다.\n",
    "        # num_train_epochs = 1, # 전체 학습을 위해 이 값을 1로 설정합니다.\n",
    "        max_steps = 60, # 최대 단계를 60으로 설정합니다.\n",
    "        learning_rate = 2e-4, # 학습률을 2e-4로 설정합니다.\n",
    "        fp16 = not is_bfloat16_supported(), # bfloat16 지원 여부에 따라 fp16을 설정합니다.\n",
    "        bf16 = is_bfloat16_supported(), # bfloat16 지원 여부에 따라 bf16을 설정합니다.\n",
    "        logging_steps = 1, # 로그 기록 단계를 1로 설정합니다.\n",
    "        optim = \"adamw_8bit\", # 옵티마이저를 adamw_8bit로 설정합니다.\n",
    "        weight_decay = 0.01, # 가중치 감소를 0.01로 설정합니다.\n",
    "        lr_scheduler_type = \"linear\", # 학습률 스케줄러 유형을 linear로 설정합니다.\n",
    "        seed = 3407, # 시드를 3407로 설정합니다.\n",
    "        output_dir = \"outputs\", # 출력 디렉토리를 \"outputs\"로 설정합니다.\n",
    "    ),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title 현재 메모리 상태 표시\n",
    "# torch.cuda.get_device_properties(0)을 사용하여 첫 번째 GPU의 속성을 가져옵니다.\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "\n",
    "# torch.cuda.max_memory_reserved()를 사용하여 현재 예약된 최대 GPU 메모리를 가져오고,\n",
    "# 이를 기가바이트(GB) 단위로 변환하여 반올림합니다.\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "# gpu_stats.total_memory를 사용하여 GPU의 총 메모리를 가져오고,\n",
    "# 이를 기가바이트(GB) 단위로 변환하여 반올림합니다.\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "# GPU 이름과 총 메모리 크기를 출력합니다.\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "\n",
    "# 현재 예약된 메모리 크기를 출력합니다.\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e402b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title 학습 시작\n",
    "trainer_stats = trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title 최종 메모리 및 시간 통계 표시\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f00c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 추론\n",
    "모델을 실행해 봅시다! 지시 사항과 입력을 변경할 수 있습니다 - 출력을 비워 두세요!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # 2배 빠른 추론을 활성화합니다.\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"기사문을 만들어주세요.\", # instruction\n",
    "        \"코난쌤TV 유튜브 채널 3000명 돌파\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\") # 입력을 텐서 형식으로 변환하고 GPU로 이동합니다.\n",
    "\n",
    "# 모델을 사용하여 텍스트를 생성합니다.\n",
    "# 최대 64개의 새로운 토큰을 생성하고 캐시를 사용하도록 설정합니다.\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "\n",
    "# 생성된 출력을 디코딩하여 텍스트 형식으로 변환합니다.\n",
    "tokenizer.batch_decode(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd253247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "`TextStreamer` 기능 사용한 코드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad35ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"기사문을 만들어주세요.\", # instruction\n",
    "        \"코난쌤TV 유튜브 채널 3000명 돌파\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# 텍스트 스트리머를 설정합니다.\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4bcf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 미세조정된 모델 저장 및 로딩\n",
    "최종 모델을 LoRA 어댑터로 저장하려면, 온라인 저장을 위해 Huggingface의 `push_to_hub`를 사용하거나 로컬 저장을 위해 `save_pretrained`를 사용하세요.\n",
    "\n",
    "**[주의]** 이는 LoRA 어댑터만 저장하며, 전체 모델을 저장하지 않습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # 허깅페이스에 저장하기\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # 허깅페이스에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abddfe77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a96378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c9cd8c7",
   "metadata": {},
   "source": [
    "\n",
    "# 채팅 인터페이스 만들기\n",
    "- gradio를 사용합니다 [gradio docs](https://www.gradio.app/docs/gradio/interface)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adbd979",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "# FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate_response(message, history):\n",
    "    instruction = message\n",
    "    input_text = \"\"  # You can modify this if you want to use chat history\n",
    "\n",
    "    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{0}\n",
    "\n",
    "### Input:\n",
    "{1}\n",
    "\n",
    "### Response:\n",
    "{2}\"\"\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            alpaca_prompt.format(\n",
    "                instruction,  # instruction\n",
    "                input_text,   # input\n",
    "                \"\",           # output - leave this blank for generation!\n",
    "            )\n",
    "        ],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "    # text_streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    generated_text = \"\"\n",
    "    with torch.no_grad(): #PyTorch에서 제공하는 컨텍스트 매니저, 추론 시 그래디언트 계산을 비활성화하여 메모리 사용량 감소 및 속도 향상\n",
    "        for token in model.generate(**inputs, streamer=text_streamer, max_new_tokens=1024):\n",
    "            next_token = tokenizer.decode(token)\n",
    "            generated_text += next_token\n",
    "            yield generated_text\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.ChatInterface(\n",
    "    generate_response,\n",
    "    title=\"AI Assistant\",\n",
    "    description=\"채팅을 시작합니다. 메시지를 입력하세요.\",\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009579ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
